# OSC-LLM

è½»é‡çº§å¤§æ¨¡å‹æ¨ç†å·¥å…·ï¼Œé«˜å¹¶å‘ï¼Œä½å»¶è¿Ÿï¼Œæ˜“æ‹“å±•ã€‚

## ç‰¹æ€§

- **CUDA Graph**: ç¼–è¯‘ä¼˜åŒ–ï¼Œå‡å°‘æ¨ç†å»¶è¿Ÿã€‚
- **PagedAttention**: é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†ï¼Œæ”¯æŒé«˜å¹¶å‘ã€‚
- **è¿ç»­æ‰¹å¤„ç†**: æ”¯æŒåŠ¨æ€æ‰¹é‡æ¨ç†ä¼˜åŒ–ã€‚
- **FlashAttention**: é•¿åºåˆ—æ˜¾å­˜ä¼˜åŒ–ã€‚

> ğŸ’¡ æ‰€æœ‰æŠ€æœ¯ç»†èŠ‚å‡åŸºäº[osc-transformers](https://github.com/di-osc/osc-transformers)æ„å»ºï¼Œè¯¦æƒ…è¯·å‰å¾€æŸ¥çœ‹ã€‚

## å®‰è£…

- å®‰è£…[pytorch](https://pytorch.org/)
- å®‰è£…[flash-attn](https://github.com/Dao-AILab/flash-attention): å»ºè®®ä¸‹è½½å®˜æ–¹æ„å»ºå¥½çš„whlåŒ…ï¼Œé¿å…ç¼–è¯‘é—®é¢˜
- å®‰è£…osc-llm
```bash
pip install osc-llm --upgrade
```

## å¿«é€Ÿå¼€å§‹


### åŸºæœ¬ä½¿ç”¨

```python
from osc_llm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM("checkpoints/Qwen/Qwen3-0.6B", gpu_memory_utilization=0.5, device="cuda:0")

# å¯¹è¯
messages = [
    {"role": "user", "content": "ä½ å¥½å•Šï¼Œä½ å«ä»€ä¹ˆ?"}
]
sampling_params = SamplingParams(temperature=0.5, top_p=0.95, top_k=40)
result = llm.chat(messages=messages, sampling_params=sampling_params, enable_thinking=True, stream=False)
print(result)

# æµå¼ç”Ÿæˆ
for token in llm.chat(messages=messages, sampling_params=sampling_params, enable_thinking=True, stream=True):
    print(token, end="", flush=True)
```

## æ”¯æŒçš„æ¨¡å‹

- Qwen3ForCausalLM
- Qwen2ForCausalLM

