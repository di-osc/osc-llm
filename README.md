<div align='center'>

# OSC-LLM
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://lightning.ai/docs/overview/getting-started"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>

</div>

## ğŸ“Œ&nbsp;&nbsp; ç®€ä»‹

osc-llmæ—¨åœ¨æˆä¸ºä¸€ä¸ªç®€å•æ˜“ç”¨çš„å¤§æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†ã€éƒ¨ç½²å·¥å…·ï¼Œæ”¯æŒä¸»æµçš„å¤§æ¨¡å‹ã€‚

> æ–‡æ¡£åœ°å€:
- [notion](https://wangmengdi.notion.site/OSC-LLM-5a04563d88464530b3d32b31e27c557a)

## ğŸ“Œ&nbsp;&nbsp; å®‰è£…

- å®‰è£…[æœ€æ–°ç‰ˆæœ¬pytorch](https://pytorch.org/get-started/locally/)
- å®‰è£…osc-llm: `pip install osc-llm`

## ğŸ“Œ&nbsp;&nbsp; å¿«é€Ÿå¼€å§‹
```bash
# ä¸‹é¢ä»¥llama3ä¸ºä¾‹æ¼”ç¤ºå¦‚ä½•è½¬æ¢ä¸ºosc-llmæ ¼å¼,å¹¶è¿›è¡ŒèŠå¤©ã€‚
# å‡è®¾ä½ å·²ç»ä¸‹è½½å¥½huggingfaceçš„llama3æ¨¡å‹åœ¨checkpoints/meta-llamaç›®å½•ä¸‹
# 1. è½¬æ¢
llm convert --checkpoint_dir checkpoints/meta-llama/Meta-Llama-3-8B-Instruct
# 2. é‡åŒ–
llm quantize int8 --checkpoint_dir checkpoints/meta-llama/Meta-Llama-3-8B-Instruct --save_dir checkpoints/meta-llama/Meta-Llama-3-8B-Instruct-int8
# 3. èŠå¤©(ä½¿ç”¨ç¼–è¯‘åŠŸèƒ½åŠ é€Ÿæ¨ç†é€Ÿåº¦,éœ€è¦ç­‰å¾…å‡ åˆ†é’Ÿç¼–è¯‘æ—¶é—´)
llm chat --checkpoint_dir checkpoints/meta-llama/Meta-Llama-3-8B-Instruct-int8 --compile true
# 4. éƒ¨ç½²
llm serve --checkpoint_dir checkpoints/meta-llama/Meta-Llama-3-8B-Instruct-int8
```

## ğŸ“Œ&nbsp;&nbsp; æ¨¡å‹æ”¯æŒ

ä»¥ä¸‹huggingfaceä¸­çš„æ¨¡å‹ç»“æ„(æŸ¥çœ‹config.json)å·²ç»æ”¯æŒè½¬æ¢ä¸ºosc-llmæ ¼å¼:
- **LlamaForCausalLM**: llama2, llama3, chinese-alpaca2ç­‰ã€‚
- **Qwen2ForCausalLM**: qwen1.5ç³»åˆ—ã€‚
- **Qwen2MoeForCausalLM**: qwen2-moeç³»åˆ—(ç›®å‰æ— æ³•å®Œæˆç¼–è¯‘,æ¨ç†é€Ÿåº¦å¾ˆæ…¢)ã€‚


### è‡´æ•¬
æœ¬é¡¹ç›®å‚è€ƒäº†å¤§é‡çš„å¼€æºé¡¹ç›®ï¼Œç‰¹åˆ«æ˜¯ä»¥ä¸‹é¡¹ç›®ï¼š

- [litgpt](https://github.com/Lightning-AI/litgpt)
- [gpt-fast](https://github.com/pytorch-labs/gpt-fast)