# OSC-LLM

è½»é‡çº§å¤§æ¨¡å‹æ¨ç†å·¥å…·ï¼Œä¸“æ³¨äºæ¨¡å‹æ¨ç†å»¶è¿Ÿã€‚

## ç‰¹æ€§

### ğŸš€ é«˜æ€§èƒ½æ¨ç†
- **CUDA Graph**: ç¼–è¯‘ä¼˜åŒ–ï¼Œå‡å°‘æ¨ç†å»¶è¿Ÿ
- **PagedAttention**: é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†ï¼Œæ”¯æŒé•¿åºåˆ—æ¨ç†
- **è¿ç»­æ‰¹å¤„ç†**: æ”¯æŒåŠ¨æ€æ‰¹é‡æ¨ç†ä¼˜åŒ–

### ğŸ› ï¸ æ˜“ç”¨æ€§
- **è½»é‡çº§è®¾è®¡**: ä¸“æ³¨äºæ¨ç†æ€§èƒ½ï¼Œå‡å°‘ä¾èµ–
- **ç®€å•API**: ç®€æ´çš„Pythonæ¥å£
- **æ¨¡å‹ç®¡ç†**: å†…ç½®ä¸‹è½½å’Œç®¡ç†å·¥å…·

## å®‰è£…

- å®‰è£…[æœ€æ–°ç‰ˆæœ¬pytorch](https://pytorch.org/)
- å®‰è£…[flash-attn](https://github.com/Dao-AILab/flash-attention): å»ºè®®ä¸‹è½½å®˜æ–¹æ„å»ºå¥½çš„whlåŒ…ï¼Œé¿å…ç¼–è¯‘é—®é¢˜
- å®‰è£…osc-llm
```bash
pip install osc-llm --upgrade
```

## å¿«é€Ÿå¼€å§‹

### ä¸‹è½½æ¨¡å‹

```bash
llm download Qwen/Qwen3-0.6B
```

### åŸºæœ¬ä½¿ç”¨

```python
from osc_llm import Qwen3ForCausalLM

# åˆå§‹åŒ–æ¨¡å‹
llm = Qwen3ForCausalLM("checkpoints/Qwen/Qwen3-0.6B")
llm.setup(device="cuda:0", gpu_memory_utilization=0.9)

# å¯¹è¯
chat_template = llm.get_chat_template()
chat_template.add_user_message("ä»‹ç»ä¸€ä¸‹åŒ—äº¬")
prompt = chat_template.apply(enable_thinking=True)
assistant_content = llm.generate(prompts=[prompt])[0]
chat_template.add_assistant_message(assistant_content)
print(chat_template.messages)
```

### æµå¼ç”Ÿæˆ

```python
chat_template = llm.get_chat_template()
chat_template.add_user_message("ä»‹ç»ä¸€ä¸‹åŒ—äº¬")
prompt = chat_template.apply(enable_thinking=True)
for token in llm.stream(prompt=prompt):
    print(token, end="", flush=True)
```

## æ”¯æŒçš„æ¨¡å‹

- Qwen3ForCausalLM
- Qwen2ForCausalLM

## CLI å·¥å…·

```bash
llm download <repo_id> [--endpoint hf-mirror|modelscope]
```